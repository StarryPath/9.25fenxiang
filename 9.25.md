#9月25日分享
###第一阶段：
	1、实现的功能：实现一个网页，在网页上输入url，爬虫就会开始爬该网页的<head><body><title>标签中的中文内容，并将爬到的内容和输入url一并存入数据库中。
![](https://raw.githubusercontent.com/StarryPath/9.25fenxiang/master/9.25-001.png) 
2、主要代码：
a.正则选择中文：

       def parse(self, response):
           
            item=PhpspiderItem()

            item['title'] = response.xpath('/html/head/title').re(u'[\u4e00-\u9fa5]')
            item['head'] = response.xpath('/html/head').re(u'[\u4e00-\u9fa5]')
            item['body'] =response.xpath('/html/body').re(u'[\u4e00-\u9fa5]')
            item['body']=  ''.join(item['body'])
            item['head'] = ''.join(item['head'])
            item['title'] = ''.join(item['title'])


            yield item
            
 
 
 b.手动输入参数（url）：
 
	     def __init__(self,category=None,*args,**kwargs):
	        super(phpSpider, self).__init__(*args, **kwargs)
	        self.start_urls = ["%s" % category]
	        
	            
c.php执行系统命令：

	$url=$_POST["url"];
	
	$program="sudo scrapy crawl phpspider -a category=".$url." -o a.json";
	
	exec($program);
	
此处执行系统命令的用户是www-data，由于权限问题，一些操作比如插入数据库会失败，所以需要将www-data的权限提高，使其可以执行sudo命令。

![](https://raw.githubusercontent.com/StarryPath/9.25fenxiang/master/9.25002.png) 
###第二阶段：

1、实现功能：从数据库中读取url，进行爬取，再入库。
2、spider.py的代码:
	
	class DmozSpider(scrapy.Spider):
	    name = "dmoz"
	    global urls,url
	    urls=[]
	    try:
	        con = MySQLdb.connect(
	                host='localhost',
	                port=3306,
	                user='root',
	                passwd='',
	                db='test'
	                )
	        print "connect db successfully!"
	
	    except:
	        print "connect db failed!"
	    with con:
	        cur=con.cursor()
	        cur.execute("select siteName from grabsite")
	        rows=cur.fetchall()
	
	        for row in rows:
	            url = str(row)[2:-3]
	            urls.append(url)
	
	    start_urls=urls[:]
	
	    urls.reverse()
	    print start_urls
	    def parse(self, response):
	
	
	            item=DmozItem()
	
	            item['title'] = response.xpath('/html/head/title').re(u'[\u4e00-\u9fa5]')
	            item['head'] = response.xpath('/html/head').re(u'[\u4e00-\u9fa5]')
	            item['body'] =response.xpath('/html/body').re(u'[\u4e00-\u9fa5]')
	            item['body']=  ''.join(item['body'])
	            item['head'] = ''.join(item['head'])
	            item['title'] = ''.join(item['title'])
	            item['Url']=urls.pop()
	
	            yield item
	            
	            
3.更新数据库：
	   
	    def insert_into_table(self, conn, item):
	
	            sql = "update grabsite set title=%s,head=%s,body=%s where siteName=%s"
	            param = ([item['title'], item['head'],item['body'],item['Url']])
	            conn.execute(sql, param)
	
4.待解决的问题：
我本以为可以实现预期的功能，但是：
a、由于scrapy爬虫是异步爬取的，效率虽然可以保证，但是顺序是乱的：
![](https://raw.githubusercontent.com/StarryPath/9.25fenxiang/master/9.25-003.png) 

b、404，503等无法访问网站的情况没有入库。

